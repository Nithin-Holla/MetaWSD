meta_learner: baseline
meta_model: seq
learner_model: bert
learner_params:
  hidden_size: 192
  num_outputs:
    wsd: 5612
  embed_dim: 768
  dropout_ratio: 0
vectors: bert
output_lr: 0.1
learner_lr: 0.00005
num_shots:
  wsd: 8
num_updates: 7
num_test_samples:
  wsd: 8
num_train_episodes:
  wsd: 10000
num_val_episodes:
  wsd: 167
num_test_episodes:
  wsd: 264
num_meta_epochs: 25
early_stopping: 2
device: cuda:0
proto_maml: false
